{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.parsers import ParserError\n",
    "import numpy as np\n",
    "from helper import get_mapper\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, stat\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from calendar import isleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SIZE = 512\n",
    "BASE_DIR = \".\"\n",
    "FIX_DIR = \"fix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fix_files(BASE_DIR):\n",
    "    tmp = [f for f in os.listdir(BASE_DIR) if os.path.isfile(join(BASE_DIR, f))]\n",
    "    tmp.sort()\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_folder(folder):\n",
    "    onlyfiles = [folder + \"/\" + f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    onlyfiles.sort()\n",
    "    files = [f for f in onlyfiles if stat(f).st_size > MIN_SIZE]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERS = [os.path.join(BASE_DIR, o) for o in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR,o))]\n",
    "FOLDERS.sort()\n",
    "FOLDERS = FOLDERS[1:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = (365 * 5 + 1) * 24\n",
    "base = datetime.datetime(2015, 1, 1)\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(0, hours)]\n",
    "COMPLETE = pd.DataFrame(data={'produced_at': date_list})\n",
    "COMPLETE.produced_at = pd.to_datetime(COMPLETE.produced_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./2015', './2016_17', './2018_19']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = get_mapper('plantmapper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_L = [get_files_from_folder(f) for f in FOLDERS]\n",
    "FILES = [item for sublist in FILES_L for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(col):\n",
    "    return col.split(\"Generation_DE \")[1].rsplit('[MW]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_powers(dirs):\n",
    "    dfs = []\n",
    "    for d in dirs:\n",
    "        print(d)\n",
    "        files = get_files_from_folder(d)\n",
    "        tmp = gen_power(files)\n",
    "        dfs.append(tmp)\n",
    "        print(dfs)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msg(tup):\n",
    "    msg = \"\"\n",
    "    corr_dates, corr_power = tup\n",
    "    if corr_dates:\n",
    "        msg += \"Dates wrong\"\n",
    "    elif corr_power:\n",
    "        msg += \"Power wrong\"\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dirs(dirs):\n",
    "    errored_files = []\n",
    "    for d in dirs:\n",
    "        print(d)\n",
    "        files = get_files_from_folder(d)\n",
    "        for file in files:\n",
    "            try:\n",
    "                df = pd.read_csv(file, sep=\";\")#, na_values=0)\n",
    "                result, errtup = is_valid_df(df)\n",
    "                if not result:\n",
    "                    errored_files.append((file, get_msg(errtup)))\n",
    "            except ParserError:\n",
    "                errored_files.append((file, \"ParserError\"))\n",
    "    return errored_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_correct_dates(df, name):\n",
    "    count = df.loc[df[name].apply(lambda x: len(x.split(\".\"))) != 3].shape[0]\n",
    "    if count == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_correct_power(df):\n",
    "    cols = list(df)[2:]\n",
    "    #print(cols)\n",
    "    map_dict = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        map_dict[col] = str\n",
    "        \n",
    "        \n",
    "    df2 = df.astype(map_dict)\n",
    "    \n",
    "    res = True\n",
    "    \n",
    "    for col in cols:\n",
    "        count = df2.loc[df2[col].apply(lambda x: len(x.split(\":\"))) != 1].shape[0]\n",
    "        if count == 0:\n",
    "            res = res and True\n",
    "        else:\n",
    "            res = res and False\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_df(df):\n",
    "    result = True\n",
    "    date, tod = (\"Date\", \"Time of day\")\n",
    "    if \"Date\" not in list(df):\n",
    "        date, tod = (\"Datum\", \"Uhrzeit\")\n",
    "    \n",
    "    corr_dates = df_correct_dates(df, date)\n",
    "    corr_power = df_correct_power(df)\n",
    "    \n",
    "    result = result and corr_dates\n",
    "    result = result and corr_power\n",
    "    return (result, (corr_dates, corr_power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_from_file(s):\n",
    "    return s.rsplit('_2018', 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plant(f):\n",
    "    return f.split(\"/\")[2].rsplit(\"_\", 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(plant, cols):\n",
    "    map_dict = {}\n",
    "    to_delete = []\n",
    "    for c in cols:\n",
    "        block = \"\"\n",
    "        try:\n",
    "            block = mapper[plant][get_block(c)]\n",
    "        except KeyError:\n",
    "            print(\"KeyError\")\n",
    "            print(plant)\n",
    "            print(cols)\n",
    "            print(c)\n",
    "        if block:\n",
    "            map_dict[c] = block\n",
    "        else:\n",
    "            to_delete.append(c)\n",
    "            \n",
    "    return map_dict, to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_dt(df):\n",
    "    date, tod = (\"Date\", \"Time of day\")\n",
    "    #print(df.shape)\n",
    "    #print(list(df))\n",
    "    if \"Date\" not in list(df):\n",
    "        date, tod = (\"Datum\", \"Uhrzeit\")\n",
    "    df[\"produced_at\"] = df[date] + \" \" + df[tod]\n",
    "    df[\"produced_at\"] = pd.to_datetime(df['produced_at'], errors='coerce')\n",
    "    df = df.drop(columns=[date, tod])\n",
    "    df2 = df.drop_duplicates([\"produced_at\"])\n",
    "    df3 = df2.dropna(subset=['produced_at']) # remove coerced errors\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_to_blockid(df, name):\n",
    "    cols = list(df)[:-1]\n",
    "    map_dict, to_delete = get_columns(name, cols)\n",
    "    df2 = df.rename(columns=map_dict)\n",
    "    df3 = df2.drop(columns=to_delete)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_path(file):\n",
    "    tmp = file.split(\"/\")\n",
    "    return \"fix/\" + tmp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_dict(array):\n",
    "    res = {}\n",
    "    for a in array:\n",
    "        res[a] = str\n",
    "    return res\n",
    "\n",
    "def get_int_dict(array):\n",
    "    res = {}\n",
    "    for a in array:\n",
    "        res[a] = int\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_num(x):\n",
    "    if isinstance(x, float):\n",
    "        return int(x)\n",
    "    elif str(x).isnumeric():\n",
    "        return x\n",
    "    else:\n",
    "        return \"\".join(re.findall(r\"\\d\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_date_df(years):\n",
    "    hours = 0\n",
    "    baseyear = years[0]\n",
    "    for year in years:\n",
    "        days = 366 if isleap(year) else 365\n",
    "        hours += days * 24\n",
    "    base = datetime.datetime(baseyear, 1, 1)\n",
    "    date_list = [base + datetime.timedelta(hours=x) for x in range(0, hours)]\n",
    "    datedf = pd.DataFrame(data={'produced_at': date_list})\n",
    "    datedf.produced_at = pd.to_datetime(datedf.produced_at)\n",
    "    return datedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_to_power(df):\n",
    "    #df = fix_df(df)\n",
    "    final = df.dropna(subset=[\"produced_at\"])\n",
    "    powers = final.melt(id_vars=[\"produced_at\"], var_name='blockid', value_name='power')\n",
    "    powers2 = powers.copy()\n",
    "    #powers2['power'] = powers2['power'].fillna(0)\n",
    "    #powers2.power.replace(['-'], [0], inplace=True)\n",
    "    #powers2 = powers2.astype({\"power\": str})\n",
    "    #powers2['power'] = powers2['power'].apply(lambda x: x.replace(\".\", \"\"))\n",
    "    powers2['power'] = powers2['power'].fillna(0)\n",
    "    powers2 = powers2.astype({\"power\": int})\n",
    "    #powers3 = powers2.copy()\n",
    "    return powers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_power(filelist):\n",
    "    df = gen_df(filelist)\n",
    "    return conv_to_power(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_power3(files):\n",
    "    result = COMPLETE\n",
    "    for plant, files in FILES_DICT.items():\n",
    "        final = pd.DataFrame()\n",
    "        for idx, f in enumerate(files):\n",
    "            f = \"fix/\" + f\n",
    "            df = pd.read_csv(f, parse_dates=[\"produced_at\"])\n",
    "            df2 = df.merge(DATEDF_LIST[idx], on='produced_at', how='right')\n",
    "            if final.empty:\n",
    "                final = df2\n",
    "            else:\n",
    "                final = final.append(df2, sort=False)\n",
    "        result = pd.merge(result, final, how='left', on=['produced_at'])\n",
    "    return result.sort_values(by=['produced_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_power2(files):\n",
    "    result = COMPLETE\n",
    "    final = pd.DataFrame()\n",
    "    for idx, f in enumerate(files):\n",
    "        f = \"fix/\" + f\n",
    "        df = pd.read_csv(f, parse_dates=[\"produced_at\"])\n",
    "        #print()\n",
    "        #df.produced_at = pd.to_datetime(df.produced_at)\n",
    "        #print(df.dtypes)\n",
    "        #print(DATEDF_LIST[idx].dtypes)\n",
    "        #df2 = df.merge(DATEDF_LIST[idx], on='produced_at')\n",
    "        df2 = df.merge(DATEDF_LIST[idx], on='produced_at', how='right') # use left for not filling missing values with na\n",
    "        #print(df.shape)\n",
    "        if final.empty:\n",
    "            final = df2\n",
    "        else:\n",
    "            final = final.append(df2)\n",
    "        final.sort_values(by=['produced_at'], inplace=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(f):\n",
    "    return f.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_df_from_file(f):\n",
    "    return gen_date_df(get_years(get_year(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(yearstr):\n",
    "    if yearstr == \"2015\":\n",
    "        return [2015]\n",
    "    elif yearstr == \"2016_17\":\n",
    "        return [2016, 2017]\n",
    "    elif yearstr == \"2018_19\":\n",
    "        return [2018, 2019]\n",
    "    else:\n",
    "        raise ValueError(\"wrong data \" + yearstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fix_filename(f):\n",
    "    return f.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_to_int(df):\n",
    "    headers = list(df)\n",
    "    headers.remove('produced_at')\n",
    "    df[headers] = df[headers].fillna(0)\n",
    "    df = df.astype(get_int_dict(headers))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df(df):\n",
    "    headers = list(df)\n",
    "    headers.remove('produced_at')\n",
    "    #print(headers)\n",
    "    powers2 = df.copy()\n",
    "    powers2 = powers2.astype(get_str_dict(headers))\n",
    "    powers2[headers] = powers2[headers].fillna(\"0\")\n",
    "    #powers2[headers] = powers2[headers].applymap(lambda x: int(x) if str(x).isnumeric() else x) # cast floats to int\n",
    "    powers2[headers] = powers2[headers].applymap(lambda x: \"0\" if not str(x).isnumeric() and \":\" in x else x) # remove dates from int column\n",
    "    powers2[headers] = powers2[headers].applymap(lambda x: x if str(x).isnumeric() else \"\".join(re.findall(r\"\\d\", x)) or 0) # remove . in ints\n",
    "    powers2[headers] = powers2[headers].applymap(lambda x: x if not (str(x).isnumeric() and len(str(x)) < 4) else int(str(x)[0:4])) # trunc to first 4 digits\n",
    "    powers2[headers] = powers2[headers].fillna(0)\n",
    "    #powers2[headers] = powers2[headers].replace(r'^\\s*$', 0, regex=True) # replace emptystrings with zero\n",
    "    powers3 = powers2.copy()\n",
    "    powers3 = powers3.astype(get_int_dict(headers))\n",
    "    #powers3['produced_at'] = pd.to_datetime(powers3['produced_at'])\n",
    "    return powers3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df(filelist):\n",
    "    #powers = pd.DataFrame()\n",
    "    tmp = COMPLETE\n",
    "    for file in filelist[:]:\n",
    "        \n",
    "        refdf = get_date_df_from_file(file)\n",
    "        \n",
    "        #print(file)\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=\";\", na_values=[\"-\", ''])#, na_values=0)\n",
    "            cols = list(df)\n",
    "            str_cols = cols[2:len(cols)]\n",
    "            dtdict = {}\n",
    "            for s in str_cols:\n",
    "                dtdict[s] = str\n",
    "            df = pd.read_csv(file, sep=\";\", na_values=[\"-\", ''], dtype=dtdict)#, na_values=0)\n",
    "        except (ParserError, UnicodeDecodeError):\n",
    "            print(file)\n",
    "            continue\n",
    "        name = get_plant(file)\n",
    "        try:\n",
    "            df2 = conv_to_dt(df)\n",
    "        except (ParserError, TypeError) as e:\n",
    "            continue\n",
    "        df3 = rename_to_blockid(df2, name)\n",
    "        df4 = pd.merge(refdf, df3, how='left', on=['produced_at']) # fill nan values\n",
    "        df5 = fix_df(df4)\n",
    "        #return df5\n",
    "        fixp = fix_path(file)\n",
    "        df6 = df5.sort_values(by=['produced_at'])\n",
    "        df6.to_csv(fixp, index=False)\n",
    "        if tmp.empty:\n",
    "            tmp = df6 # merge with itself if no other exists\n",
    "        tmp = pd.merge(tmp, df6, how='left', on=['produced_at'])\n",
    "        tmp2 = fill_to_int(tmp)\n",
    "        #old = tmp.copy()\n",
    "    return tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_df(filelist, err_bl=True):\n",
    "    #powers = pd.DataFrame()\n",
    "    tmp = COMPLETE\n",
    "    for file in filelist[:]:\n",
    "        \n",
    "        refdf = get_date_df_from_file(file)\n",
    "        \n",
    "        #print(file)\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=\";\", na_values=[\"-\", ''], error_bad_lines=err_bl, nrows=5)#, na_values=0)\n",
    "            cols = list(df)\n",
    "            str_cols = cols[2:len(cols)]\n",
    "            use_cols = ['Datum', 'Uhrzeit'] + str_cols\n",
    "            dtdict = {}\n",
    "            for s in str_cols:\n",
    "                dtdict[s] = str\n",
    "            df = pd.read_csv(file, sep=\";\", na_values=[\"-\", ''], dtype=dtdict, error_bad_lines=err_bl)#, na_values=0)\n",
    "        except (ParserError, UnicodeDecodeError) as e:\n",
    "            print(e)\n",
    "            print(file)\n",
    "            continue\n",
    "        name = get_plant(file)\n",
    "        try:\n",
    "            df2 = conv_to_dt(df)\n",
    "        except (ParserError, TypeError) as e:\n",
    "            continue\n",
    "        df3 = rename_to_blockid(df2, name)\n",
    "        df4 = pd.merge(refdf, df3, how='left', on=['produced_at']) # fill nan values\n",
    "        df5 = fix_df(df4)\n",
    "        #return df5\n",
    "        fixp = fix_path(file)\n",
    "        df6 = df5.sort_values(by=['produced_at'])\n",
    "        df6.to_csv(fixp, index=False)\n",
    "        if tmp.empty:\n",
    "            tmp = df6 # merge with itself if no other exists\n",
    "        tmp = pd.merge(tmp, df6, how='left', on=['produced_at'])\n",
    "        tmp2 = fill_to_int(tmp)\n",
    "        #old = tmp.copy()\n",
    "    return tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = get_mapper('plantmapper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = gen_df(FILES, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7d4a69619bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-5416cc0cc38a>\u001b[0m in \u001b[0;36mgen_df\u001b[0;34m(filelist, err_bl)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_plant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_to_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-1e501e70789e>\u001b[0m in \u001b[0;36mconv_to_dt\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Datum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Uhrzeit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"produced_at\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"produced_at\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'produced_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"produced_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, box, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mallow_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         )\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown string format: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"String does not contain a date: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         return (sum(getattr(self, attr) is not None\n\u001b[0;32m--> 241\u001b[0;31m                     for attr in self.__slots__))\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/idp/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         return (sum(getattr(self, attr) is not None\n\u001b[0;32m--> 241\u001b[0;31m                     for attr in self.__slots__))\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parta = gen_df(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partb = melt_to_power(parta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testfiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-f6a1dd2750fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFILES_DICT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_20\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFILES_DICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testfiles' is not defined"
     ]
    }
   ],
   "source": [
    "FILES_DICT = {}\n",
    "for f in testfiles:\n",
    "    name = f.split(\"_20\")[0]\n",
    "    if not name in FILES_DICT:\n",
    "        tmp = [f]\n",
    "        FILES_DICT[name] = tmp\n",
    "    else:\n",
    "        tmp = FILES_DICT[name]\n",
    "        tmp = tmp + [f]\n",
    "        FILES_DICT[name] = tmp\n",
    "    #FILES_DICT[name] = []\n",
    "    #print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATEDF_LIST = [gen_date_df(year) for year in [[2015], [2016, 2017], [2018, 2019]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF = gen_power3(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF.sort_values(by='produced_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF.loc[CDF['BNA1404']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = [\"./2016_17/Buschhaus_201601010000_201712312345_146.csv\", \"./2016_17/Brokdorf_201601010000_201712312345_150.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = gen_df(F2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sort_values(by=\"BNA0439\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sort_values(by=\"BNA0439\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([DATEDF_LIST[0], test1, test1], sort=False).drop_duplicates(subset=['produced_at'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfiles = get_fix_files(FIX_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATEDF_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF = parta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF = pd.read_csv(\"CDF.csv\", parse_dates=['produced_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF2 = melt_to_power(CDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF = pd.read_csv(\"produced_power.csv\", parse_dates=['produced_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "produced_at    object\n",
       "BNA0067         int64\n",
       "BNA1404         int64\n",
       "BNA0124         int64\n",
       "BNA0123         int64\n",
       "                ...  \n",
       "BNA0413c        int64\n",
       "BNA1071         int64\n",
       "BNA1092         int64\n",
       "BNA1091         int64\n",
       "BNA1093         int64\n",
       "Length: 250, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>produced_at</th>\n",
       "      <th>BNA0067</th>\n",
       "      <th>BNA1404</th>\n",
       "      <th>BNA0124</th>\n",
       "      <th>BNA0123</th>\n",
       "      <th>BNA0122</th>\n",
       "      <th>BNA0115</th>\n",
       "      <th>BNA0116</th>\n",
       "      <th>BNA0157</th>\n",
       "      <th>BNA0172a</th>\n",
       "      <th>...</th>\n",
       "      <th>BNA1025</th>\n",
       "      <th>BNA0413b</th>\n",
       "      <th>BNA0415</th>\n",
       "      <th>BNA0414</th>\n",
       "      <th>BNA0413a</th>\n",
       "      <th>BNA0413c</th>\n",
       "      <th>BNA1071</th>\n",
       "      <th>BNA1092</th>\n",
       "      <th>BNA1091</th>\n",
       "      <th>BNA1093</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43819</th>\n",
       "      <td>2019-12-31 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>490</td>\n",
       "      <td>808</td>\n",
       "      <td>0</td>\n",
       "      <td>1326</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43820</th>\n",
       "      <td>2019-12-31 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>478</td>\n",
       "      <td>500</td>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>1325</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43821</th>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>477</td>\n",
       "      <td>502</td>\n",
       "      <td>857</td>\n",
       "      <td>0</td>\n",
       "      <td>1326</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43822</th>\n",
       "      <td>2019-12-31 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>477</td>\n",
       "      <td>503</td>\n",
       "      <td>858</td>\n",
       "      <td>0</td>\n",
       "      <td>1326</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43823</th>\n",
       "      <td>2019-12-31 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>481</td>\n",
       "      <td>502</td>\n",
       "      <td>851</td>\n",
       "      <td>0</td>\n",
       "      <td>1325</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43824 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               produced_at  BNA0067  BNA1404  BNA0124  BNA0123  BNA0122  \\\n",
       "0      2015-01-01 00:00:00        0        0        0        0        0   \n",
       "1      2015-01-01 01:00:00        0        0        0        0        0   \n",
       "2      2015-01-01 02:00:00        0        0        0        0        0   \n",
       "3      2015-01-01 03:00:00        0        0        0        0        0   \n",
       "4      2015-01-01 04:00:00        0        0        0        0        0   \n",
       "...                    ...      ...      ...      ...      ...      ...   \n",
       "43819  2019-12-31 19:00:00        0        0        0      475      490   \n",
       "43820  2019-12-31 20:00:00        0        0        0      478      500   \n",
       "43821  2019-12-31 21:00:00        0        0        0      477      502   \n",
       "43822  2019-12-31 22:00:00        0       63        0      477      503   \n",
       "43823  2019-12-31 23:00:00        0      213        0      481      502   \n",
       "\n",
       "       BNA0115  BNA0116  BNA0157  BNA0172a  ...  BNA1025  BNA0413b  BNA0415  \\\n",
       "0            0        0        0         0  ...        0         0        0   \n",
       "1            0        0        0         0  ...        0         0        0   \n",
       "2            0        0        0         0  ...        0         0        0   \n",
       "3            0        0        0         0  ...        0         0        0   \n",
       "4            0        0        0         0  ...        0         0        0   \n",
       "...        ...      ...      ...       ...  ...      ...       ...      ...   \n",
       "43819      808        0     1326         0  ...      306         0        0   \n",
       "43820      850        0     1325         0  ...      305         0        0   \n",
       "43821      857        0     1326         0  ...      306         0        0   \n",
       "43822      858        0     1326         0  ...      307         0        0   \n",
       "43823      851        0     1325         0  ...      308         0        0   \n",
       "\n",
       "       BNA0414  BNA0413a  BNA0413c  BNA1071  BNA1092  BNA1091  BNA1093  \n",
       "0            0         0         0        0        0        0        0  \n",
       "1            0         0         0        0        0        0        0  \n",
       "2            0         0         0        0        0        0        0  \n",
       "3            0         0         0        0        0        0        0  \n",
       "4            0         0         0        0        0        0        0  \n",
       "...        ...       ...       ...      ...      ...      ...      ...  \n",
       "43819        0         0         0        0        0        0        0  \n",
       "43820        0         0         0        0        0        0        0  \n",
       "43821        0         0         0        0        0        0        0  \n",
       "43822        0         0         0        0        0        0        0  \n",
       "43823        0         0         0        0        0        0        0  \n",
       "\n",
       "[43824 rows x 250 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"produced_power_pg.csv\", index=True, header=False)\n",
    "df.to_csv(\"produced_power.csv\", index=False)\n",
    "df.to_csv(\"produced_power_nh.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>produced_at</th>\n",
       "      <th>blockid</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912171</th>\n",
       "      <td>2019-12-31 19:00:00</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912172</th>\n",
       "      <td>2019-12-31 20:00:00</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912173</th>\n",
       "      <td>2019-12-31 21:00:00</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912174</th>\n",
       "      <td>2019-12-31 22:00:00</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912175</th>\n",
       "      <td>2019-12-31 23:00:00</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10912176 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  produced_at  blockid  power\n",
       "0         2015-01-01 00:00:00  BNA0067      0\n",
       "1         2015-01-01 01:00:00  BNA0067      0\n",
       "2         2015-01-01 02:00:00  BNA0067      0\n",
       "3         2015-01-01 03:00:00  BNA0067      0\n",
       "4         2015-01-01 04:00:00  BNA0067      0\n",
       "...                       ...      ...    ...\n",
       "10912171  2019-12-31 19:00:00  BNA1093      0\n",
       "10912172  2019-12-31 20:00:00  BNA1093      0\n",
       "10912173  2019-12-31 21:00:00  BNA1093      0\n",
       "10912174  2019-12-31 22:00:00  BNA1093      0\n",
       "10912175  2019-12-31 23:00:00  BNA1093      0\n",
       "\n",
       "[10912176 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF.to_csv(\"CDF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['produced_at', 'blockid', 'power']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gy = CDF.resample('1Y', on='produced_at').sum()\n",
    "gm = CDF.resample('1M', on='produced_at').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2 = gm.reset_index()\n",
    "gm2['year'] = gm2['produced_at']\n",
    "gm2['month'] = gm2['produced_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2['year'] = gm2['year'].apply(lambda x: str(x).split(\"-\")[0])\n",
    "gm2['month'] = gm2['month'].apply(lambda x: int(str(x).split(\"-\")[1]))\n",
    "gm2['month'] = gm2['month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm3 = gm2.sort_values(by=[\"year\", 'month'])\n",
    "gm4 = gm3.drop(columns='produced_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm5 = gm4.melt(id_vars=[\"year\", \"month\"], var_name='blockid', value_name='power')\n",
    "gm5['power'] = gm5['power'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>blockid</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>70621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>68088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>66709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>53328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>BNA0067</td>\n",
       "      <td>76938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>58135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>58514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14937</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>110960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14938</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>151811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>BNA1093</td>\n",
       "      <td>69385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14940 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  blockid   power\n",
       "0      2015      1  BNA0067   70621\n",
       "1      2015      2  BNA0067   68088\n",
       "2      2015      3  BNA0067   66709\n",
       "3      2015      4  BNA0067   53328\n",
       "4      2015      5  BNA0067   76938\n",
       "...     ...    ...      ...     ...\n",
       "14935  2019      8  BNA1093   58135\n",
       "14936  2019      9  BNA1093   58514\n",
       "14937  2019     10  BNA1093  110960\n",
       "14938  2019     11  BNA1093  151811\n",
       "14939  2019     12  BNA1093   69385\n",
       "\n",
       "[14940 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm5.to_csv(\"monthly.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm4.to_csv(\"monthly.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2[headers] = powers2[headers].applymap(lambda x: \"0\" if not str(x).isnumeric() and \":\" in x else x) # remove dates from int column\n",
    "powers2[headers] = powers2[headers].applymap(lambda x: x if str(x).isnumeric() else \"\".join(re.findall(r\"\\d\", x)) or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2 = gm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF2.groupby(CDF2['produced_at'])CDF2.groupby(CDF2['produced_at']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = CDF.duplicated(subset=['produced_at'])\n",
    "#CDF[mask].sort_values(by=['produced_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = testfiles[15:18]\n",
    "testdf = gen_power2(FT)\n",
    "#testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set subtraction\n",
    "#pd.concat([COMPLETE, df, df]).drop_duplicates(subset=['produced_at'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = gen_df([FILES[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = gen_power([\"./2016_17/Heizkraftwerk_Altbach_Deizisau_201601010000_201712312345_10.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq = failed.duplicated(subset=['produced_at', 'blockid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed[mq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (365 * 5 + 1) * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = datetime.datetime(2015, 1, 1)\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(0, a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list[a-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedf = pd.DataFrame(data={'producet_at': date_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv(\"./fix/./2015/Boxberg_201501010000_201512312345_71.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = \"1.234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = r\"\\d\"\n",
    "r2 = r\"\\\\d\"\n",
    "r3 = r\"\\\\\\d\"\n",
    "r4 = \"\\\\d\"\n",
    "r5 = \"\\\\\\\\d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = re.findall(r\"\\d\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist = ['BNA0104', 'BNA0124']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[alist] = tdf[alist].replace(['-'], [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = validate_dirs(FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = gen_powers(FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df['produced_at'] > \"2015-01-01 00:00:00\") & (df['produced_at'] <= \"2017-01-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"produced_at\", \"blockid\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.duplicated(subset=[\"produced_at\", \"blockid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = get_files_from_folder(FOLDERS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk = fs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.read_csv(bk, sep=\";\")#, na_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, tod = (\"Datum\", \"Uhrzeit\")\n",
    "tf[\"produced_at\"] = tf[date] + \" \" + tf[tod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.loc[tf.duplicated(['produced_at'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sort_values(by=['Datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[\"produced_at\"] = pd.to_datetime(tf['produced_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2 = conv_to_dt(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F15 = FOLDERS[2]\n",
    "F16 = FOLDERS[0]\n",
    "F18 = FOLDERS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['produced_at', 'blockid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.read_csv(\"faulty.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = validate_dirs(FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_df(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.loc[ft['Datum'].apply(lambda x: len(x.split(\".\"))) != 3].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen_powers(FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./2015/Bergkamen_201501010000_201512312345_28.csv\", sep=\";\")#, na_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_from_folder(F15)\n",
    "final = gen_df(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KeyError\n",
    "Heizkraftwerk_Dresden-Nossener_Br_cke\n",
    "['Generation_DE Heizkraftwerk Dresden-Nossener Brücke ']\n",
    "Generation_DE Heizkraftwerk Dresden-Nossener Brücke \n",
    "ParserError\n",
    "./2015/Kraftwerk_BASF_Ludwigshafen_Mitte_201501010000_201512312345_20.csv\n",
    "KeyError\n",
    "Kraftwerk_BASF_Ludwigshafen_Mitte\n",
    "['Generation_DE Koepchenwerk[MW]']\n",
    "Generation_DE Koepchenwerk[MW]\n",
    "KeyError\n",
    "Kraftwerk_BASF_Ludwigshafen_S_d\n",
    "['Generation_DE GUD C 200']\n",
    "Generation_DE GUD C 200\n",
    "KeyError\n",
    "Kraftwerk_West\n",
    "['Generation_DE West 2[MW]', 'Generation_DE West 1[MW]']\n",
    "Generation_DE West 2[MW]\n",
    "KeyError\n",
    "Kraftwerk_West\n",
    "['Generation_DE West 2[MW]', 'Generation_DE West 1[MW]']\n",
    "Generation_DE West 1[MW]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = get_name_from_file(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = pd.read_csv(files[2], sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final = gen_df(F15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = conv_to_power(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.dropna(subset=[\"produced_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = final.melt(id_vars=[\"produced_at\"], var_name='blockid', value_name='power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers2 = powers.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers2['power'] = powers2['power'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers2.power.replace(['-'], [0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers3 = powers2.astype({\"power\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers3.to_csv(\"produced_power_pg.csv\", index=True, header=False)\n",
    "powers3.to_csv(\"produced_power.csv\", index=False)\n",
    "powers3.to_csv(\"produced_power_nh.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(powers3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers.blockid.str.len().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = powers['blockid'].str.len() == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://simon:\"N0m1596.\"@localhost:5432/power')\n",
    "\n",
    "powers.to_sql(\"power\", engine, if_exists=\"replace\", method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#powers.groupby(\"Blockid\")['Blockid'].apply(lambda x: x.str.len(x).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = pd.read_csv(files[1], sep=\";\")\n",
    "df = pd.read_csv(files[2], sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = conv_to_dt(df)\n",
    "df3 = rename_to_blockid(df2, \"Braunkohlekraftwerk_Lippendorf\")\n",
    "dq2 = conv_to_dt(dq)\n",
    "dq3 = rename_to_blockid(dq2, \"Boxberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = dq3[\"Datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq3[ids.isin(ids[ids.duplicated()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dq3.duplicated([\"Datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq3.drop_duplicates([\"Datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = pd.merge(dq3, df3, how='outer', on=['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot.drop_duplicates([\"Datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = pd.concat([dq3, df3], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Error tokenizing data. C error: Expected 3 fields in line 2355, saw 4\n",
    "\n",
    "./2016_17/Cuno_Heizkraftwerk_Herdecke_201601010000_201712312345_17.csv\n",
    "Error tokenizing data. C error: Expected 4 fields in line 7786, saw 9\n",
    "\n",
    "./2016_17/Duisburg_Heizkraftwerk_III_201601010000_201712312345_18.csv\n",
    "Error tokenizing data. C error: Expected 3 fields in line 5981, saw 4\n",
    "\n",
    "./2016_17/Gemeinschaftskraftwerk_Kiel_201601010000_201712312345_7.csv\n",
    "Error tokenizing data. C error: Expected 4 fields in line 5861, saw 5\n",
    "\n",
    "./2016_17/Huckingen_201601010000_201712312345_6.csv\n",
    "Error tokenizing data. C error: Expected 5 fields in line 13947, saw 7\n",
    "\n",
    "./2016_17/Kraftwerk_BASF_Ludwigshafen_Mitte_201601010000_201712312345_20.csv\n",
    "Error tokenizing data. C error: Expected 5 fields in line 7403, saw 9\n",
    "\n",
    "./2016_17/Kraftwerk_Mittelsb_ren_201601010000_201712312345_4.csv\n",
    "Error tokenizing data. C error: Expected 5 fields in line 8430, saw 6\n",
    "\n",
    "./2016_17/Kraftwerk_Werdohl-Elverlingsen_201601010000_201712312345_15.csv\n",
    "Error tokenizing data. C error: Expected 3 fields in line 15454, saw 5\n",
    "\n",
    "./2016_17/Kraftwerk_Wilhelmshaven_201601010000_201712312345_13.csv\n",
    "Error tokenizing data. C error: Expected 4 fields in line 1721, saw 8\n",
    "\n",
    "./2016_17/Reuter_West_201601010000_201712312345_16.csv\n",
    "Error tokenizing data. C error: Expected 4 fields in line 7454, saw 11\n",
    "\n",
    "./2016_17/Tiefstack_201601010000_201712312345_31.csv\n",
    "Error tokenizing data. C error: Expected 3 fields in line 14633, saw 4\n",
    "\n",
    "./2016_17/Trianel_Kohlekraftwerk_L_nen_201601010000_201712312345_26.csv\n",
    "Error tokenizing data. C error: Expected 4 fields in line 5877, saw 6\n",
    "\n",
    "./2016_17/Waldeck_2_201601010000_201712312345_32.csv\n",
    "Error tokenizing data. C error: Expected 3 fields in line 7138, saw 10\n",
    "\n",
    "./2016_17/Wehr_201601010000_201712312345_3.csv\n",
    "Error tokenizing data. C error: Expected 3 fields in line 3689, saw 4\n",
    "\n",
    "./2016_17/Weiher_201601010000_201712312345_21.csv\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
